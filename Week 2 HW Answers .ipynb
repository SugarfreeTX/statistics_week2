{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This is displaying data cleaning, formatting, Stop Words, Word Count and Use of Dictionaries using Quakedata.csv and moviedata.csv without using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Load the MovieData.csv dataset as described in this week's lesson, and use it to find the following values:\n",
    "\n",
    "a. What is the average US Gross of movies in the dataset?\n",
    "\n",
    "b. How many movies in the dataset have budgets greater than $20 million?\n",
    "\n",
    "c. How many movies were released by each film distributor? (Hint: this could be a good place to use dictionaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"moviedata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release_Date\tMovie\tDistributor\tBudget\tUS Gross\tWorldwide Gross\n",
      "\n",
      "03/09/12\tJohn Carter\t\t300000000\t66439100\t254439100\n",
      "\n",
      "05/25/07\tPirates of the Caribbean: At World's End\tBuena Vista\t300000000\t309420425\t960996492\n",
      "\n",
      "12/13/13\tThe Hobbit: There and Back Again\tNew Line\t270000000\tUnknown\tUnknown\n",
      "\n",
      "12/14/12\tThe Hobbit: An Unexpected Journey\tNew Line\t270000000\tUnknown\tUnknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in range(5):\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to format the date column and convert all currency values to int. \n",
    "# Create functions. \n",
    "import datetime as dt \n",
    "\n",
    "# Convert string to int. \n",
    "def convert_to_int(text):\n",
    "    ''' Convert a string into an integer'''\n",
    "    try:\n",
    "        return int(text)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Convert string to datetime object so we can use if needed.\n",
    "def make_date(date_str):\n",
    "    '''Turn mm/dd/yy into datetime object.'''\n",
    "    m, d, y = date_str.split(\"/\")\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    y = int(y)\n",
    "    if y > 13:\n",
    "        y += 1900\n",
    "    else:\n",
    "        y += 2000\n",
    "    return dt.datetime(y, m, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release_Date\tMovie\tDistributor\tBudget\tUS Gross\tWorldwide Gross\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply to the dataset and build a list called data. \n",
    "file = open('moviedata.csv')\n",
    "data = []\n",
    "print(file.readline()) # skip the first row since it's the headers. \n",
    "for row in file:\n",
    "    row = row.split('\\t')\n",
    "    row[0] = make_date(row[0])\n",
    "    row[3] = convert_to_int(row[3])\n",
    "    row[4] = convert_to_int(row[4])\n",
    "    row[5] = convert_to_int(row[5])\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the average US Gross of movies in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44185793.44224795\n"
     ]
    }
   ],
   "source": [
    "total = 0.0 # make it a float to make division work\n",
    "count = 0 # Increase by 1 for each row with a gross. \n",
    "for row in data:\n",
    "    try: \n",
    "        total += row[4]\n",
    "        count += 1\n",
    "    except:\n",
    "        pass # do nothing if there is a None entry.\n",
    "\n",
    "print(total/count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. How many movies in the dataset have budgets greater than $20 million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657\n"
     ]
    }
   ],
   "source": [
    "big_budget = 0 # Create a counter\n",
    "for row in data:\n",
    "    if row[3] > 20000000: # anything greater than 20 million \n",
    "        big_budget += 1\n",
    "print(big_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. How many movies were released by each film distributor? (Hint: this could be a good place to use dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 659\n",
      "Buena Vista 227\n",
      "New Line 138\n",
      "Sony 312\n",
      "Warner Bros. 311\n",
      "20th Century Fox 230\n",
      "Sony Pictures 2\n",
      "Paramount Pictures 258\n",
      "Universal 261\n",
      "Walt Disney Co. 9\n",
      "Columbia 26\n",
      "DreamWorks SKG 78\n",
      "MGM/UA 137\n",
      "Miramax 135\n",
      "Lionsgate 13\n",
      "Polygram Films 5\n",
      "Weinstein Co. 33\n",
      "United Artists 23\n",
      "Summit 11\n",
      "Lion's Gate 79\n",
      "TriStar Pictures 4\n",
      "DEJ Productions 1\n",
      "Warner Independent 3\n",
      "Weinstein 1\n",
      "Dimension Films 2\n",
      "Focus Features 36\n",
      "Third Rail 2\n",
      "Sony/Tristar 1\n",
      "Paramount Vantage 3\n",
      "Orion 17\n",
      "USA Films 15\n",
      "Dimension 28\n",
      "Sony Classics 78\n",
      "Magnolia 15\n",
      "Artisan 23\n",
      "Summit Entertainment 3\n",
      "MGM 3\n",
      "RS Entertainment 1\n",
      "CBS Films 3\n",
      "Freestyle 2\n",
      "Senator Films 1\n",
      "Fox Searchlight 65\n",
      "Warner Independent Pictures 7\n",
      "Sony/Gems 1\n",
      "Sony/Screen Gems 1\n",
      "Relativity 1\n",
      "New Market 7\n",
      "Rogue 1\n",
      "FilmDistrict 1\n",
      "ThinkFilm 9\n",
      "Gramercy 12\n",
      "Overture 6\n",
      "Savoy 3\n",
      "8 X Entertainment 1\n",
      "Fine Line 16\n",
      "Walt Disney Pictures 1\n",
      "Paramount Classics 12\n",
      "Destination 4\n",
      "Cloud Ten Pictures 1\n",
      "Filmways 2\n",
      "Weintraub 2\n",
      "IFC Films 20\n",
      "Galactic 1\n",
      "Film Foundry 1\n",
      "Samuel Goldwyn 9\n",
      "Picturehouse 6\n",
      "Alliance 4\n",
      "Disney 1\n",
      "Barking Cow 1\n",
      "Weinstein/Dimension 1\n",
      "Kino 5\n",
      "Destination Films 2\n",
      "Strand 8\n",
      "Newmarket Films 1\n",
      "Music Box 1\n",
      "First Look 9\n",
      "Consolidated Pictures Group 1\n",
      "October 6\n",
      "IDP 2\n",
      "Live Entertainment 3\n",
      "Screen Gems 1\n",
      "Embassy 1\n",
      "Cannon 4\n",
      "WinStar 1\n",
      "Goldwyn 6\n",
      "Black Diamond Pictures 1\n",
      "Eros 5\n",
      "First Independent Pictures 1\n",
      "Paladin 1\n",
      "WellSpring 2\n",
      "Roadside Attractions 4\n",
      "New Yorker 2\n",
      "Anchor Bay 4\n",
      "Universal/Rogue 1\n",
      "Apparition 4\n",
      "Strand Releasing 2\n",
      "Weinstein Ci. 1\n",
      "Cinema Service 1\n",
      "Trimark 9\n",
      "PION 1\n",
      "Wellspring 1\n",
      "Excel Entertainment 5\n",
      "Providence 2\n",
      "Giant 1\n",
      "Freestyle/Darko 1\n",
      "Roadside 2\n",
      "Indican 7\n",
      "Inerstar 1\n",
      "Yash Raj 1\n",
      "Avco Embassy 5\n",
      "Triumph 1\n",
      "Lorimar 2\n",
      "RKO 2\n",
      "Zeitgeist 7\n",
      "Oscilloscope 2\n",
      "Palm/Manga 1\n",
      "Lions Gate/IFC Films/Fellowship Adventure Group 1\n",
      "3D Entertainment 1\n",
      "Galaxy 1\n",
      "Cowboy 7\n",
      "October Films 1\n",
      "American International Pictures 1\n",
      "LIONS 1\n",
      "New Century Vista Film Company 1\n",
      "Hemdale Film Coorporation 1\n",
      "Tartan 1\n",
      "Palm Pictures 3\n",
      "Big Pictures 1\n",
      "IDP/Gold Circle 1\n",
      "Independent Artists 1\n",
      "Overture Films 1\n",
      "Empire 1\n",
      "Universal/Arenas Entertainment 1\n",
      "Phaedra 2\n",
      "New World 5\n",
      "CHRIST 1\n",
      "United Film Distribution 2\n",
      "Legacy 1\n",
      "Atlantic 1\n",
      "Regent Releasing 1\n",
      "Good Machine 1\n",
      "Attitude Films 1\n",
      "Odeon 1\n",
      "Shooting Gallery 1\n",
      "Small Planet 1\n",
      "Palisades Entertainment 1\n",
      "Access 1\n",
      "MORO 1\n",
      "Island/Alive 1\n",
      "Off Hollywood Pictures 1\n",
      "Artistic License 1\n",
      "IDP/Goldwyn 2\n",
      "Samuel Goldwyn Films 1\n",
      "Monterey Media 1\n",
      "New Films Int'l 1\n",
      "Bigger Picture 1\n",
      "NORTH 1\n",
      "Lions Gate 1\n",
      "David Keith Co. 1\n",
      "NEW LTN 1\n",
      "Roxie Releasing 1\n",
      "Videos 1\n",
      "Stratosphere 1\n",
      "Vitagraph Films 2\n",
      "Film Movement 2\n",
      "RBC Radio, LLC 1\n",
      "Romar 1\n",
      "Regent 1\n",
      "Rogue Pictures 1\n",
      "TLA Releasing 1\n",
      "Five and Two Pictures 1\n",
      "Rainforest Films 1\n",
      "Film Sales Co. 1\n",
      "Magnolia Pictures 1\n",
      "First Look Pictures 1\n",
      "Lot 47 1\n",
      "RED HOR 1\n",
      "The Movie Partners 1\n",
      "OpenEdge Media 1\n",
      "Mulberry Square Releasing 1\n",
      "J.F. Prods 1\n",
      "Halestorm 1\n",
      "Halestorm Entertainment 1\n",
      "CFP 1\n",
      "Fabrication Films 1\n",
      "New World Pictures 1\n",
      "Zion 1\n",
      "ART 1\n",
      "Painted Zebra Releasing 1\n",
      "Shotwell Media 1\n",
      "Outrider Pictures 1\n",
      "RAIN 1\n",
      "Island 1\n",
      "Testimony Pictures 1\n",
      "INWOO 1\n",
      "Off-Hollywood Distribution 1\n",
      "Jerry Gross Organization 1\n",
      "Avatar 1\n",
      "IDP/Sam Goldwyn 1\n",
      "Power Point 1\n",
      "IDP/Stratosphere 1\n",
      "Damiano 1\n",
      "Orion Classics 1\n",
      "JeTi Films 1\n",
      "Lavender House 1\n",
      "Cinema con Sabor 1\n",
      "Winstar 1\n",
      "Truly Indie 1\n",
      "CustomFlix 1\n"
     ]
    }
   ],
   "source": [
    "distributors = {}\n",
    "for row in data: \n",
    "    distributor = row[2] # Distributor column \n",
    "    if distributor in distributors:\n",
    "        distributors[distributor] += 1 # add to existing distributor\n",
    "    else:\n",
    "        distributors[distributor] = 1 # create new with 1 count \n",
    "        \n",
    "# Print results. \n",
    "for distributor in distributors:\n",
    "    print(distributor, distributors[distributor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Load the earthquake data in QuakeData.csv, and use it to answer the following questions:\n",
    "\n",
    "a. How many earthquakes are in the dataset?\n",
    "\n",
    "b. What is the average magnitude of earthquakes in the dataset?\n",
    "\n",
    "c. The DateTime format in this dataset is a bit trickier than it was for movies. Try to parse it into a datetime object. Do most earthquakes happen between midnight and noon, or noon to midnight?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime,Latitude,Longitude,Depth,Magnitude,MagType,NbStations,Gap,Distance,RMS,Source,EventID,Version\n",
      "\n",
      "2012-01-01T00:30:08.770+00:00,12.008,143.487,35.0,5.1,mb,178,45,,1.20,pde,pde20120101003008770_35,1363392487731\n",
      "\n",
      "2012-01-01T00:43:42.770+00:00,12.014,143.536,35.0,4.4,mb,29,121,,0.98,pde,pde20120101004342770_35,1363392488431\n",
      "\n",
      "2012-01-01T00:50:08.040+00:00,-11.366,166.218,67.5,5.3,mb,143,43,,0.82,pde,pde20120101005008040_67,1363392488479\n",
      "\n",
      "2012-01-01T01:22:07.660+00:00,-6.747,130.008,145.0,4.2,mb,14,112,,1.16,pde,pde20120101012207660_145,1363392488594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quake = open('quakedata.csv')\n",
    "for row in range(5):\n",
    "    print(quake.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is seperated by commas and the date format is showing dates and time. \n",
    "Will also need to convert the records into float numbers and return string if it fails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to convert record into floating point and return string if fails.\n",
    "def convert(record):\n",
    "    try:\n",
    "        return float(record)\n",
    "    except:\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DateTime', 'Latitude', 'Longitude', 'Depth', 'Magnitude', 'MagType', 'NbStations', 'Gap', 'Distance', 'RMS', 'Source', 'EventID', 'Version\\n']\n"
     ]
    }
   ],
   "source": [
    "qdata = []\n",
    "quake = open('quakedata.csv')\n",
    "print(quake.readline().split(',')) # print the column headers \n",
    "for row in quake:\n",
    "    row = row.split(',') # split the commas\n",
    "    row = [convert(x) for x in row] # use list comp to conver each entry\n",
    "    qdata.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. How many earthquakes are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12684\n"
     ]
    }
   ],
   "source": [
    "print(len(qdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is the average magnitude of earthquakes in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.558483128350625\n"
     ]
    }
   ],
   "source": [
    "# row[4]\n",
    "mags = [row[4] for row in qdata] # making a list of all mags\n",
    "total = sum(mags) * 1.0 # multiply by 1.0 to make sure it's a float value. \n",
    "print(total/len(mags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. The DateTime format in this dataset is a bit trickier than it was for movies. Try to parse it into a datetime object. Do most earthquakes happen between midnight and noon, or noon to midnight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2012-01-01T05:19:34.930+00:00'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to see a date and then use it to test with a function for conversion. \n",
    "test_date = qdata[10][0]\n",
    "test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 01 01 05 19 34\n"
     ]
    }
   ],
   "source": [
    "# Will need to parse out the date and then convert it to a datetime object\n",
    "year = test_date[:4]\n",
    "month = test_date[5:7]\n",
    "day = test_date[8:10]\n",
    "hour = test_date[11:13]\n",
    "minute = test_date[14:16]\n",
    "second = test_date[17:19]\n",
    "print(year, month, day, hour, minute, second, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to convert the dates to datetime objects.\n",
    "def make_datetime(record):\n",
    "    year = int(record[:4])\n",
    "    month = int(record[5:7])\n",
    "    day = int(record[8:10])\n",
    "    hour = int(record[11:13])\n",
    "    minute = int(record[14:16])\n",
    "    second = int(record[17:19])\n",
    "    return dt.datetime(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-01 05:19:34\n"
     ]
    }
   ],
   "source": [
    "# Test function on the test_date.\n",
    "print(make_datetime(test_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6241\n",
      "6443\n"
     ]
    }
   ],
   "source": [
    "# Now to answer the question. \n",
    "quake_times = [make_datetime(row[0]) for row in qdata]\n",
    "morning_count = 0\n",
    "evening_count = 0\n",
    "for time in quake_times: \n",
    "    if time.hour < 12:\n",
    "        morning_count += 1\n",
    "    else: \n",
    "        evening_count += 1\n",
    "print(morning_count)\n",
    "print(evening_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "a) Read in the text in the file, and count how many times each individual word appears. For this exercise, words are separated by whitespace -- don't worry about colons, dashes, etc.\n",
    "\n",
    "b) Stop words are words that appear so often in a language that they aren't useful for analysis (you may have noticed them in your results for a). Below is a list of stop words taken from NLTK, the Natural Language Toolkit for Python. Read in the document and count words again -- but this time, convert all the words to lower-case, and only include the words that aren't on the stop word list. Also remove any punctuation (the characters .,?!-) from the beginning and end of words Finally, only output the words which appear more than once.\n",
    "\n",
    "c) Write the results of the previous section (including words that appear only once) to a csv file. There should be two columns, one for the word and the other for the count. For example:\n",
    "\n",
    "Word,Count\n",
    "million,2\n",
    "objectives,1\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "STOP_WORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Read in the text in the file, and count how many times each individual word appears. For this exercise, words are separated by whitespace -- don't worry about colons, dashes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to use .split() to split on white space since it's a txt\n",
    "# Use a dictionary to count how many times the words appear\n",
    "# Loop over each row and seperate each row into words. \n",
    "\n",
    "text = open('data.txt')\n",
    "word_counts = {}\n",
    "for row in text:\n",
    "    for word in row.split():\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview 1\n",
      "Our 5\n",
      "mission 1\n",
      "is 8\n",
      "to 33\n",
      "give 1\n",
      "people 12\n",
      "the 12\n",
      "power 1\n",
      "share 4\n",
      "and 42\n",
      "make 2\n",
      "world 2\n",
      "more 1\n",
      "open 1\n",
      "connected. 1\n",
      "business 3\n",
      "focuses 2\n",
      "on 19\n",
      "creating 1\n",
      "value 2\n",
      "for 6\n",
      "people, 1\n",
      "marketers, 2\n",
      "developers. 2\n",
      "How 3\n",
      "We 10\n",
      "Create 3\n",
      "Value 3\n",
      "People 1\n",
      "Who 1\n",
      "Use 1\n",
      "Facebook 13\n",
      "top 1\n",
      "priority 1\n",
      "build 1\n",
      "useful 1\n",
      "engaging 1\n",
      "products 1\n",
      "that 6\n",
      "enable 5\n",
      "connect 1\n",
      "through 1\n",
      "mobile 13\n",
      "devices 3\n",
      "personal 6\n",
      "computers. 5\n",
      "also 5\n",
      "help 6\n",
      "discover 1\n",
      "learn 1\n",
      "about 1\n",
      "what 1\n",
      "going 1\n",
      "in 8\n",
      "around 1\n",
      "them, 1\n",
      "their 14\n",
      "opinions, 1\n",
      "ideas, 1\n",
      "photos 2\n",
      "videos, 2\n",
      "other 3\n",
      "activities 1\n",
      "with 8\n",
      "audiences 1\n",
      "ranging 1\n",
      "from 11\n",
      "closest 1\n",
      "friends 2\n",
      "public 1\n",
      "at 1\n",
      "large, 1\n",
      "stay 1\n",
      "connected 1\n",
      "everywhere 1\n",
      "by 4\n",
      "accessing 1\n",
      "our 7\n",
      "products, 1\n",
      "including: 1\n",
      "Facebook. 1\n",
      "The 1\n",
      "app 1\n",
      "website 1\n",
      "connect, 1\n",
      "share, 1\n",
      "discover, 1\n",
      "communicate 1\n",
      "each 2\n",
      "free 1\n",
      "available 3\n",
      "throughout 1\n",
      "world. 1\n",
      "had 2\n",
      "890 1\n",
      "million 2\n",
      "daily 1\n",
      "active 1\n",
      "users 1\n",
      "(DAUs) 1\n",
      "average 2\n",
      "December 4\n",
      "2014 2\n",
      ", 2\n",
      "an 5\n",
      "increase 3\n",
      "of 13\n",
      "18% 1\n",
      "compared 2\n",
      "2013 1\n",
      ". 1\n",
      "745 1\n",
      "DAUs 1\n",
      "who 6\n",
      "accessed 1\n",
      "a 9\n",
      "device 1\n",
      "34% 1\n",
      "2013. 1\n",
      "Instagram. 1\n",
      "Instagram 2\n",
      "application 7\n",
      "enables 2\n",
      "take 1\n",
      "or 5\n",
      "customize 1\n",
      "them 6\n",
      "filter 1\n",
      "effects, 1\n",
      "followers 1\n",
      "photo 1\n",
      "feed 1\n",
      "send 1\n",
      "directly 1\n",
      "friends. 1\n",
      "Messenger. 1\n",
      "Messenger 3\n",
      "mobile-to-mobile 1\n",
      "messaging 3\n",
      "Android, 2\n",
      "iOS 1\n",
      "Windows 2\n",
      "Phone 1\n",
      "devices. 3\n",
      "works 1\n",
      "similarly 1\n",
      "texting 1\n",
      "(SMS) 1\n",
      "online 4\n",
      "chat 1\n",
      "reach 2\n",
      "others 1\n",
      "instantly 1\n",
      "seamlessly 1\n",
      "integrates 1\n",
      "functionality 1\n",
      "WhatsApp. 1\n",
      "WhatsApp 1\n",
      "cross-platform 1\n",
      "allows 1\n",
      "exchange 1\n",
      "messages 1\n",
      "iOS, 1\n",
      "BlackBerry, 1\n",
      "Phone, 1\n",
      "Nokia 1\n",
      "Marketers 3\n",
      "providing 3\n",
      "all 1\n",
      "kinds 1\n",
      "including 3\n",
      "brand, 1\n",
      "direct 1\n",
      "response, 1\n",
      "small 1\n",
      "medium-sized 1\n",
      "businesses, 1\n",
      "achieve 1\n",
      "objectives, 1\n",
      "whether 1\n",
      "it 1\n",
      "driving 1\n",
      "sales, 2\n",
      "in-store 2\n",
      "awareness 1\n",
      "brand. 1\n",
      "generate 3\n",
      "substantial 1\n",
      "majority 1\n",
      "revenue 4\n",
      "selling 1\n",
      "advertising 1\n",
      "placements 1\n",
      "marketers. 1\n",
      "ads 9\n",
      "let 1\n",
      "marketers 4\n",
      "based 2\n",
      "variety 1\n",
      "factors 1\n",
      "age, 1\n",
      "gender, 1\n",
      "location, 1\n",
      "interests. 1\n",
      "purchase 2\n",
      "can 4\n",
      "appear 1\n",
      "multiple 1\n",
      "places 1\n",
      "News 1\n",
      "Feed 1\n",
      "computers, 1\n",
      "right-hand 1\n",
      "side 1\n",
      "ad 6\n",
      "planning 1\n",
      "tools 3\n",
      "are 2\n",
      "designed 1\n",
      "align 1\n",
      "marketers' 1\n",
      "goals. 1\n",
      "When 1\n",
      "create 2\n",
      "campaign 1\n",
      "Facebook, 2\n",
      "they 2\n",
      "specify 1\n",
      "budget, 1\n",
      "marketing 1\n",
      "objectives 1\n",
      "types 1\n",
      "want 1\n",
      "reach. 1\n",
      "Facebook's 1\n",
      "serving 1\n",
      "technology 1\n",
      "then 1\n",
      "dynamically 1\n",
      "determines 1\n",
      "best 1\n",
      "show 2\n",
      "person 1\n",
      "those 2\n",
      "dimensions. 1\n",
      "use 4\n",
      "platform's 1\n",
      "insights 2\n",
      "measure 1\n",
      "optimize 1\n",
      "both 1\n",
      "performance 1\n",
      "campaigns. 1\n",
      "These 1\n",
      "not 1\n",
      "only 1\n",
      "understand 1\n",
      "how 1\n",
      "drove 1\n",
      "results 1\n",
      "but 1\n",
      "modifications 1\n",
      "campaigns 1\n",
      "improve 1\n",
      "results. 1\n",
      "In 1\n",
      "addition 1\n",
      "buy 1\n",
      "websites 1\n",
      "applications 6\n",
      "such 3\n",
      "as 4\n",
      "Audience 3\n",
      "Network, 2\n",
      "Atlas, 1\n",
      "LiveRail. 1\n",
      "Developers 1\n",
      "supports 1\n",
      "developersb 1\n",
      "\u0019 1\n",
      "efforts 1\n",
      "build, 1\n",
      "grow, 1\n",
      "monetize 3\n",
      "web 4\n",
      "applications. 2\n",
      "First, 1\n",
      "we 4\n",
      "provide 1\n",
      "set 1\n",
      "development 1\n",
      "programming 1\n",
      "interfaces 1\n",
      "(APIs) 1\n",
      "developers 9\n",
      "easily 1\n",
      "integrate 1\n",
      "across 1\n",
      "platforms 1\n",
      "Second, 1\n",
      "grow 1\n",
      "tools, 1\n",
      "social 1\n",
      "plugins, 1\n",
      "exposure, 1\n",
      "distribution 1\n",
      "engagement 1\n",
      "By 1\n",
      "using 1\n",
      "sharing, 1\n",
      "messaging, 1\n",
      "invites, 1\n",
      "requests, 1\n",
      "ads, 1\n",
      "have 1\n",
      "number 1\n",
      "ways 1\n",
      "drive 1\n",
      "discovery 1\n",
      "user 1\n",
      "engagement. 1\n",
      "Finally, 1\n",
      "Payments 2\n",
      "infrastructure 2\n",
      "receive 2\n",
      "payments 1\n",
      "easy-to-use, 1\n",
      "secure, 1\n",
      "trusted 1\n",
      "environment, 1\n",
      "well 1\n",
      "where 1\n",
      "able 1\n",
      "showing 1\n",
      "advertisers 2\n",
      "within 2\n",
      "application. 1\n",
      "sell 1\n",
      "virtual 1\n",
      "digital 1\n",
      "goods 1\n",
      "choose 1\n",
      "us, 1\n",
      "portion 1\n",
      "Network. 1\n"
     ]
    }
   ],
   "source": [
    "# Loop over the dictionary to get a count for each word. \n",
    "for word in word_counts:\n",
    "    print(word, word_counts[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Stop words are words that appear so often in a language that they aren't useful for analysis (you may have noticed them in your results for a). Below is a list of stop words taken from NLTK, the Natural Language Toolkit for Python. Read in the document and count words again -- but this time, convert all the words to lower-case, and only include the words that aren't on the stop word list. Also remove any punctuation (the characters .,?!-) from the beginning and end of words Finally, only output the words which appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all words to lower case and use strip to remove special characters\n",
    "# then check for any words that match the STOP_WORDS.\n",
    "\n",
    "text = open('data.txt')\n",
    "word_counts = {}\n",
    "for row in text:\n",
    "    for word in row.split():\n",
    "        word = word.lower()\n",
    "        word = word.strip(\",.!?- \") # included space \n",
    "        if word in STOP_WORDS:\n",
    "            continue # Skip to the next word\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the words and store as a list. \n",
    "sorted_words = sorted(word_counts, key=lambda w: word_counts[w], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook 16\n",
      "people 14\n",
      "mobile 13\n",
      "developers 12\n",
      "marketers 10\n",
      "ads 10\n",
      "application 8\n",
      "applications 8\n",
      "devices 6\n",
      "personal 6\n",
      "computers 6\n",
      "help 6\n",
      "ad 6\n",
      "share 5\n",
      "value 5\n",
      "create 5\n",
      "use 5\n",
      "enable 5\n",
      "also 5\n",
      "december 4\n",
      "messenger 4\n",
      "messaging 4\n",
      "online 4\n",
      "revenue 4\n",
      "tools 4\n",
      "web 4\n",
      "world 3\n",
      "business 3\n",
      "friends 3\n",
      "available 3\n",
      " 3\n",
      "increase 3\n",
      "instagram 3\n",
      "reach 3\n",
      "providing 3\n",
      "including 3\n",
      "generate 3\n",
      "audience 3\n",
      "network 3\n",
      "monetize 3\n",
      "payments 3\n",
      "make 2\n",
      "connected 2\n",
      "focuses 2\n",
      "build 2\n",
      "products 2\n",
      "connect 2\n",
      "discover 2\n",
      "photos 2\n",
      "videos 2\n",
      "million 2\n",
      "average 2\n",
      "2014 2\n",
      "compared 2\n",
      "2013 2\n",
      "enables 2\n",
      "feed 2\n",
      "android 2\n",
      "ios 2\n",
      "windows 2\n",
      "phone 2\n",
      "whatsapp 2\n",
      "brand 2\n",
      "objectives 2\n",
      "sales 2\n",
      "in-store 2\n",
      "based 2\n",
      "purchase 2\n",
      "show 2\n",
      "insights 2\n",
      "campaigns 2\n",
      "results 2\n",
      "grow 2\n",
      "engagement 2\n",
      "infrastructure 2\n",
      "receive 2\n",
      "advertisers 2\n",
      "within 2\n"
     ]
    }
   ],
   "source": [
    "# loop over the sorted list and print out the counts for each word. \n",
    "for word in sorted_words:\n",
    "    count = word_counts[word]\n",
    "    if count > 1:\n",
    "        print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Write the results of the previous section (including words that appear only once) to a csv file. There should be two columns, one for the word and the other for the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv and add headers and repeat the loop above. \n",
    "# Eliminate white space by concatenating them into a single string\n",
    "# before writing to file. \n",
    "file = open('word_counts.txt', 'w')\n",
    "header = \"Word,Count\\n\" # line break since it's a text file\n",
    "file.write(header)\n",
    "\n",
    "for word in sorted_words:\n",
    "    count = str(word_counts[word]) # convert the number to a string\n",
    "    row = word + ',' + count + '\\n' # concatenating \n",
    "    file.write(row)\n",
    "    \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
